{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a10a444",
   "metadata": {},
   "source": [
    "# ETL RAW → SILVER: Airline On-Time / Delay Causes\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Este notebook implementa o processo de ETL (Extract, Transform, Load) para transformar os dados brutos da camada **RAW** em dados limpos e padronizados na camada **SILVER**, seguindo a arquitetura de **Medalhão** (Medallion Architecture).\n",
    "\n",
    "### O que este ETL faz?\n",
    "\n",
    "Este pipeline:\n",
    "1. **Extrai** os dados do CSV bruto (`data_raw.csv`)\n",
    "2. **Transforma** os dados aplicando limpezas, padronizações e validações\n",
    "3. **Carrega** os dados transformados na tabela `silver.silver_airline_on_time` do PostgreSQL\n",
    "\n",
    "### Por que é importante no contexto do Medalhão?\n",
    "\n",
    "- **RAW (Bronze)**: Dados brutos, sem tratamento, mantendo a fidelidade original\n",
    "- **SILVER**: Dados limpos, padronizados e validados, prontos para análise e consumo\n",
    "- **GOLD** (futuro): Dados agregados e modelados para dashboards e relatórios\n",
    "\n",
    "A camada SILVER é crucial porque:\n",
    "- Remove inconsistências que poderiam enviesar análises\n",
    "- Padroniza formatos para facilitar consultas SQL\n",
    "- Adiciona flags e metadados úteis (ex: outliers)\n",
    "- Garante integridade referencial e tipos corretos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "from __future__ import annotations  # Permite usar type hints sem importar os tipos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração de exibição\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6785ab",
   "metadata": {},
   "source": [
    "## 1. Extract - Leitura do CSV\n",
    "\n",
    "### O que fazemos aqui?\n",
    "\n",
    "Carregamos o arquivo CSV bruto da camada RAW, lidando com possíveis problemas de encoding e exibindo informações básicas sobre os dados.\n",
    "\n",
    "### Por que é importante?\n",
    "\n",
    "- Garantir que os dados sejam lidos corretamente, independente do encoding\n",
    "- Entender a estrutura inicial dos dados antes de qualquer transformação\n",
    "- Documentar o estado original para comparação posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34eb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega o CSV com tratamento de encoding.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Caminho para o arquivo CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame com os dados carregados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(\"CSV carregado com encoding UTF-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 falhou, tentando latin1...\")\n",
    "        df = pd.read_csv(file_path, encoding='latin1')\n",
    "        print(\"CSV carregado com encoding latin1\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Caminho do arquivo\n",
    "csv_path = '../Data Layer/raw/data_raw.csv'\n",
    "\n",
    "# Carregar dados\n",
    "print(\"=\" * 80)\n",
    "print(\"ETAPA 1: EXTRACT - Carregando dados do CSV\")\n",
    "print(\"=\" * 80)\n",
    "df_raw = load_csv(csv_path)\n",
    "\n",
    "# Informações básicas\n",
    "print(f\"\\nShape inicial: {df_raw.shape[0]:,} linhas × {df_raw.shape[1]} colunas\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "display(df_raw.head())\n",
    "print(f\"\\nInformações do DataFrame:\")\n",
    "df_raw.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd862e9",
   "metadata": {},
   "source": [
    "## 2. Transform - Limpeza e Padronização\n",
    "\n",
    "### O que fazemos aqui?\n",
    "\n",
    "Aplicamos uma série de transformações para limpar e padronizar os dados:\n",
    "1. Padronização de nomes de colunas\n",
    "2. Remoção de colunas 100% nulas\n",
    "3. Remoção de duplicados\n",
    "4. Conversão de tipos de dados\n",
    "5. Tratamento de valores nulos\n",
    "6. Validação de integridade (valores negativos)\n",
    "7. Detecção de outliers\n",
    "8. Normalização de campos categóricos\n",
    "\n",
    "### Por que é importante?\n",
    "\n",
    "- **Padronização de colunas**: Facilita consultas SQL e evita erros por espaços/caracteres especiais\n",
    "- **Remoção de colunas vazias**: Reduz ruído e economiza espaço\n",
    "- **Tipos corretos**: Permite operações matemáticas e agregações eficientes\n",
    "- **Tratamento de nulos**: Evita erros em cálculos e mantém consistência\n",
    "- **Flags de outliers**: Permite análises futuras sem perder dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377900cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar relatório de ETL\n",
    "etl_report = {\n",
    "    'linhas_inicial': len(df_raw),\n",
    "    'colunas_inicial': len(df_raw.columns),\n",
    "    'colunas_removidas': [],\n",
    "    'duplicados_removidos': 0,\n",
    "    'colunas_nulos_preenchidos': {},\n",
    "    'valores_negativos_corrigidos': {},\n",
    "    'outliers_detectados': 0\n",
    "}\n",
    "\n",
    "df = df_raw.copy()\n",
    "print(\"=\" * 80)\n",
    "print(\"ETAPA 2: TRANSFORM - Iniciando transformações\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eeeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Padroniza nomes de colunas: strip, lowercase, espaços por _, remove caracteres especiais.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame com colunas padronizadas\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    new_columns = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Strip, lowercase, substituir espaços por underscore\n",
    "        new_col = col.strip().lower().replace(' ', '_')\n",
    "        # Remover caracteres especiais (manter apenas letras, números e underscore)\n",
    "        new_col = ''.join(c if c.isalnum() or c == '_' else '' for c in new_col)\n",
    "        # Remover underscores múltiplos\n",
    "        while '__' in new_col:\n",
    "            new_col = new_col.replace('__', '_')\n",
    "        # Remover underscore no início/fim\n",
    "        new_col = new_col.strip('_')\n",
    "        new_columns[col] = new_col\n",
    "    \n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    print(f\"Colunas padronizadas: {len(new_columns)} colunas\")\n",
    "    return df\n",
    "\n",
    "df = standardize_columns(df)\n",
    "print(f\"\\nColunas após padronização:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover colunas 100% nulas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.1 Removendo colunas 100% nulas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "colunas_antes = set(df.columns)\n",
    "null_counts = df.isnull().sum()\n",
    "colunas_100_nulas = null_counts[null_counts == len(df)].index.tolist()\n",
    "\n",
    "if colunas_100_nulas:\n",
    "    df = df.drop(columns=colunas_100_nulas)\n",
    "    etl_report['colunas_removidas'].extend(colunas_100_nulas)\n",
    "    print(f\"Removidas {len(colunas_100_nulas)} colunas 100% nulas: {colunas_100_nulas}\")\n",
    "else:\n",
    "    print(\"Nenhuma coluna 100% nula encontrada\")\n",
    "\n",
    "print(f\"Colunas restantes: {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.2 Removendo registros duplicados\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "linhas_antes = len(df)\n",
    "df = df.drop_duplicates()\n",
    "linhas_depois = len(df)\n",
    "duplicados = linhas_antes - linhas_depois\n",
    "\n",
    "etl_report['duplicados_removidos'] = duplicados\n",
    "print(f\"Duplicados removidos: {duplicados:,} registros\")\n",
    "print(f\"Linhas restantes: {linhas_depois:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converte tipos de dados conforme regras de negócio.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a ser convertido\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame com tipos convertidos\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Converter year e month para Int64\n",
    "    if 'year' in df.columns:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "        print(\"year convertido para Int64\")\n",
    "    \n",
    "    if 'month' in df.columns:\n",
    "        df['month'] = pd.to_numeric(df['month'], errors='coerce').astype('Int64')\n",
    "        print(\"month convertido para Int64\")\n",
    "    \n",
    "    # Colunas de contagem (terminadas com _ct, _flights, _cancelled, _diverted, _del15)\n",
    "    count_patterns = ['_ct', '_flights', '_cancelled', '_diverted', '_del15']\n",
    "    count_cols = [col for col in df.columns \n",
    "                  if any(col.endswith(pattern) for pattern in count_patterns)]\n",
    "    \n",
    "    for col in count_cols:\n",
    "        if col in df.columns:\n",
    "            # Converter para float primeiro, depois arredondar e converter para Int64\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Arredondar valores decimais para inteiros (mantém NaN)\n",
    "            df[col] = df[col].round()\n",
    "            # Converter para Int64 nullable (suporta NaN)\n",
    "            df[col] = df[col].astype('Int64')\n",
    "            print(f\"{col} convertido para Int64\")\n",
    "    \n",
    "    # Colunas de delay (terminadas com _delay, arr_delay)\n",
    "    delay_patterns = ['_delay', 'arr_delay']\n",
    "    delay_cols = [col for col in df.columns \n",
    "                  if any(col.endswith(pattern) for pattern in delay_patterns) \n",
    "                  and col not in count_cols]\n",
    "    \n",
    "    for col in delay_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Float64')\n",
    "            print(f\"{col} convertido para Float64\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.3 Convertendo tipos de dados\")\n",
    "print(\"=\" * 80)\n",
    "df = cast_types(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nulls(df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Trata valores nulos: preenche com 0 em colunas numéricas de delay/contagem quando faz sentido.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a ser limpo\n",
    "        \n",
    "    Returns:\n",
    "        Tupla (DataFrame limpo, dicionário com colunas preenchidas)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    colunas_preenchidas = {}\n",
    "    \n",
    "    # Colunas numéricas de delay e contagem\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Colunas de delay e contagem que faz sentido preencher com 0\n",
    "    fill_zero_cols = [col for col in numeric_cols \n",
    "                      if any(pattern in col for pattern in ['_delay', '_ct', '_flights', '_cancelled', '_diverted', '_del15'])]\n",
    "    \n",
    "    for col in fill_zero_cols:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            df[col] = df[col].fillna(0)\n",
    "            colunas_preenchidas[col] = null_count\n",
    "            print(f\"{col}: {null_count:,} nulos preenchidos com 0\")\n",
    "    \n",
    "    return df, colunas_preenchidas\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.4 Tratando valores nulos\")\n",
    "print(\"=\" * 80)\n",
    "df, nulos_preenchidos = clean_nulls(df)\n",
    "etl_report['colunas_nulos_preenchidos'] = nulos_preenchidos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_integrity(df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Valida integridade: não permite valores negativos em contagens e delays.\n",
    "    Transforma negativos em NaN e depois em 0.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a ser validado\n",
    "        \n",
    "    Returns:\n",
    "        Tupla (DataFrame validado, dicionário com correções)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    correcoes = {}\n",
    "    \n",
    "    # Colunas de contagem e delay\n",
    "    count_delay_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                        if any(pattern in col for pattern in ['_ct', '_flights', '_cancelled', '_diverted', '_del15', '_delay'])]\n",
    "    \n",
    "    for col in count_delay_cols:\n",
    "        negativos = (df[col] < 0).sum()\n",
    "        if negativos > 0:\n",
    "            # Transformar negativos em NaN e depois em 0\n",
    "            df.loc[df[col] < 0, col] = np.nan\n",
    "            df[col] = df[col].fillna(0)\n",
    "            correcoes[col] = negativos\n",
    "            print(f\"{col}: {negativos:,} valores negativos corrigidos para 0\")\n",
    "    \n",
    "    return df, correcoes\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.5 Validando integridade (valores negativos)\")\n",
    "print(\"=\" * 80)\n",
    "df, correcoes_negativos = validate_integrity(df)\n",
    "etl_report['valores_negativos_corrigidos'] = correcoes_negativos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c236ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outlier_flags(df: pd.DataFrame, column: str = 'arr_delay') -> tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Adiciona flag de outlier usando IQR (Interquartile Range) para uma coluna específica.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        column: Nome da coluna para detectar outliers\n",
    "        \n",
    "    Returns:\n",
    "        Tupla (DataFrame com flag, número de outliers detectados)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if column not in df.columns:\n",
    "        print(f\"Coluna {column} não encontrada, pulando detecção de outliers\")\n",
    "        return df, 0\n",
    "    \n",
    "    # Calcular IQR\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Limites\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Flag de outlier\n",
    "    flag_col = f'is_outlier_{column}'\n",
    "    df[flag_col] = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    df[flag_col] = df[flag_col].astype('Int64')\n",
    "    \n",
    "    outliers_count = df[flag_col].sum()\n",
    "    \n",
    "    print(f\"Flag {flag_col} criada\")\n",
    "    print(f\"  - Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"  - Limites: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"  - Outliers detectados: {outliers_count:,} ({outliers_count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df, outliers_count\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.6 Adicionando flags de outliers\")\n",
    "print(\"=\" * 80)\n",
    "df, outliers_count = add_outlier_flags(df, 'arr_delay')\n",
    "etl_report['outliers_detectados'] = outliers_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_categoricals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza campos categóricos: strip() e uppercase em códigos.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame com campos categóricos normalizados\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Campos de identificação (códigos)\n",
    "    code_cols = ['carrier', 'airport']\n",
    "    \n",
    "    for col in code_cols:\n",
    "        if col in df.columns:\n",
    "            # Converter para string, strip, uppercase\n",
    "            df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "            print(f\"{col} normalizado (uppercase + strip)\")\n",
    "    \n",
    "    # Campos de texto (nomes)\n",
    "    text_cols = ['carrier_name', 'airport_name']\n",
    "    \n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            # Apenas strip\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            print(f\"{col} normalizado (strip)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.7 Normalizando campos categóricos\")\n",
    "print(\"=\" * 80)\n",
    "df = normalize_categoricals(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60569fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar relatório final\n",
    "etl_report['linhas_final'] = len(df)\n",
    "etl_report['colunas_final'] = len(df.columns)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMO DAS TRANSFORMAÇÕES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Linhas: {etl_report['linhas_inicial']:,} → {etl_report['linhas_final']:,}\")\n",
    "print(f\"Colunas: {etl_report['colunas_inicial']} → {etl_report['colunas_final']}\")\n",
    "print(f\"Duplicados removidos: {etl_report['duplicados_removidos']:,}\")\n",
    "print(f\"Colunas removidas: {len(etl_report['colunas_removidas'])}\")\n",
    "print(f\"Outliers detectados: {etl_report['outliers_detectados']:,}\")\n",
    "\n",
    "print(\"\\nPrimeiras linhas após transformação:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nInformações do DataFrame transformado:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 2.8 Engenharia de Dados: Criação de Período Unificado\n",
    "# ==========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2.8 Criando coluna flight_date (mantendo year e month)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df['flight_date'] = pd.to_datetime(\n",
    "    df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2) + '-01'\n",
    ")\n",
    "\n",
    "print(\"Coluna 'flight_date' criada com sucesso!\")\n",
    "print(f\"Total de colunas agora: {len(df.columns)}\")\n",
    "display(df[['year', 'month', 'flight_date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b9d14",
   "metadata": {},
   "source": [
    "## 3. Load - Carregar no PostgreSQL\n",
    "\n",
    "### O que fazemos aqui?\n",
    "\n",
    "1. Conectamos ao PostgreSQL usando SQLAlchemy\n",
    "2. Criamos o schema `silver` se não existir\n",
    "3. Criamos a tabela `silver.silver_airline_on_time` se não existir\n",
    "4. Carregamos os dados transformados na tabela\n",
    "5. Criamos índices para melhorar performance de consultas\n",
    "\n",
    "### Por que é importante?\n",
    "\n",
    "- **Conexão padronizada**: SQLAlchemy facilita migração entre SGBDs\n",
    "- **Schema organizado**: Separação lógica dos dados por camada\n",
    "- **CREATE TABLE IF NOT EXISTS**: Idempotência (pode rodar múltiplas vezes)\n",
    "- **if_exists='replace'**: Facilita reprocessamento durante desenvolvimento\n",
    "- **Índices**: Melhoram performance de consultas por year, month, carrier, airport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "POSTGRES_DB = os.getenv('POSTGRES_DB', 'airline_delay_causes')\n",
    "POSTGRES_USER = os.getenv('POSTGRES_USER', 'postgres')\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'postgres')\n",
    "POSTGRES_HOST = 'localhost'\n",
    "POSTGRES_PORT = 5432\n",
    "\n",
    "connection_string = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ETAPA 3: LOAD - Conectando ao PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Host: {POSTGRES_HOST}:{POSTGRES_PORT}\")\n",
    "print(f\"Database: {POSTGRES_DB}\")\n",
    "print(f\"User: {POSTGRES_USER}\")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(connection_string, echo=False)\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT version();\"))\n",
    "        version = result.fetchone()[0]\n",
    "        print(f\"Conectado ao PostgreSQL: {version.split(',')[0]}\")\n",
    "        \n",
    "        conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS silver;\"))\n",
    "        conn.commit()\n",
    "        print(\"Schema 'silver' criado/verificado\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ddl(df: pd.DataFrame, schema: str = 'silver', table_name: str = 'silver_airline_on_time') -> str:\n",
    "    \"\"\"\n",
    "    Gera o DDL SQL completo para criar a tabela baseado no DataFrame.\n",
    "    Inclui schema, drop table, primary key composta, comentários e constraints.\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{schema}.{table_name}\"\n",
    "    ddl_parts = []\n",
    "    \n",
    "    ddl_parts.append(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "    ddl_parts.append(\"\")\n",
    "    ddl_parts.append(f\"DROP TABLE IF EXISTS {full_table_name};\")\n",
    "    ddl_parts.append(\"\")\n",
    "    ddl_parts.append(f\"CREATE TABLE {full_table_name} (\")\n",
    "    \n",
    "    column_definitions = []\n",
    "    column_comments = []\n",
    "    not_null_cols = {'year', 'month', 'carrier', 'airport', 'arr_flights', \n",
    "                     'arr_del15', 'arr_delay', 'is_outlier_arr_delay'}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        sql_type = None\n",
    "        comment = None\n",
    "        \n",
    "        if col == 'year':\n",
    "            sql_type = \"INTEGER NOT NULL\"\n",
    "            comment = \"Ano do registro.\"\n",
    "        elif col == 'month':\n",
    "            sql_type = \"INTEGER NOT NULL\"\n",
    "            comment = \"Mês do registro (1-12).\"\n",
    "        elif col == 'carrier':\n",
    "            sql_type = \"CHAR(2) NOT NULL\"\n",
    "            comment = \"Código IATA da companhia aérea.\"\n",
    "        elif col == 'airport':\n",
    "            sql_type = \"CHAR(3) NOT NULL\"\n",
    "            comment = \"Código IATA do aeroporto.\"\n",
    "        elif col == 'carrier_name':\n",
    "            sql_type = \"VARCHAR(255)\"\n",
    "            comment = \"Nome completo da companhia aérea.\"\n",
    "        elif col == 'airport_name':\n",
    "            sql_type = \"VARCHAR(255)\"\n",
    "            comment = \"Nome completo do aeroporto.\"\n",
    "        elif col == 'arr_flights':\n",
    "            sql_type = \"INTEGER NOT NULL\"\n",
    "            comment = \"Número total de voos de chegada.\"\n",
    "        elif col == 'arr_del15':\n",
    "            sql_type = \"INTEGER NOT NULL\"\n",
    "            comment = \"Número de voos com atraso >= 15 minutos.\"\n",
    "        elif col == 'carrier_ct':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Contagem de atrasos causados pela companhia aérea.\"\n",
    "        elif col == 'weather_ct':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Contagem de atrasos causados por condições climáticas.\"\n",
    "        elif col == 'nas_ct':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Contagem de atrasos causados pelo Sistema Nacional de Espaço Aéreo (NAS).\"\n",
    "        elif col == 'security_ct':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Contagem de atrasos causados por segurança.\"\n",
    "        elif col == 'late_aircraft_ct':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Contagem de atrasos causados por aeronave atrasada.\"\n",
    "        elif col == 'arr_cancelled':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Número de voos cancelados.\"\n",
    "        elif col == 'arr_diverted':\n",
    "            sql_type = \"INTEGER\"\n",
    "            comment = \"Número de voos desviados.\"\n",
    "        elif col == 'arr_delay':\n",
    "            sql_type = \"NUMERIC(10,2) NOT NULL\"\n",
    "            comment = \"Total de minutos de atraso na chegada.\"\n",
    "        elif col == 'carrier_delay':\n",
    "            sql_type = \"NUMERIC(10,2)\"\n",
    "            comment = \"Minutos de atraso causados pela companhia aérea.\"\n",
    "        elif col == 'weather_delay':\n",
    "            sql_type = \"NUMERIC(10,2)\"\n",
    "            comment = \"Minutos de atraso causados por condições climáticas.\"\n",
    "        elif col == 'nas_delay':\n",
    "            sql_type = \"NUMERIC(10,2)\"\n",
    "            comment = \"Minutos de atraso causados pelo Sistema Nacional de Espaço Aéreo (NAS).\"\n",
    "        elif col == 'security_delay':\n",
    "            sql_type = \"NUMERIC(10,2)\"\n",
    "            comment = \"Minutos de atraso causados por segurança.\"\n",
    "        elif col == 'late_aircraft_delay':\n",
    "            sql_type = \"NUMERIC(10,2)\"\n",
    "            comment = \"Minutos de atraso causados por aeronave atrasada.\"\n",
    "        elif col == 'is_outlier_arr_delay':\n",
    "            sql_type = \"INTEGER NOT NULL\"\n",
    "            comment = \"Flag indicando se arr_delay é outlier (1=sim, 0=não, calculado via IQR).\"\n",
    "        elif col == 'flight_date':\n",
    "            sql_type = \"DATE\"\n",
    "            comment = \"Data do voo (primeiro dia do mês, derivado de year e month).\"\n",
    "        else:\n",
    "            if pd.api.types.is_integer_dtype(dtype):\n",
    "                sql_type = \"INTEGER\"\n",
    "            elif pd.api.types.is_float_dtype(dtype):\n",
    "                sql_type = \"DOUBLE PRECISION\"\n",
    "            elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "                sql_type = \"DATE\"\n",
    "            else:\n",
    "                sql_type = \"TEXT\"\n",
    "            comment = f\"Coluna {col}.\"\n",
    "        \n",
    "        if col in not_null_cols and 'NOT NULL' not in sql_type:\n",
    "            sql_type = sql_type.replace(\"INTEGER\", \"INTEGER NOT NULL\")\n",
    "            sql_type = sql_type.replace(\"NUMERIC(10,2)\", \"NUMERIC(10,2) NOT NULL\")\n",
    "        \n",
    "        column_definitions.append(f\"    {col} {sql_type}\")\n",
    "        \n",
    "        if comment:\n",
    "            column_comments.append(f\"COMMENT ON COLUMN {full_table_name}.{col} IS '{comment}';\")\n",
    "    \n",
    "    column_definitions.append(\"    CONSTRAINT pk_silver_airline_on_time PRIMARY KEY (year, month, carrier, airport)\")\n",
    "    \n",
    "    ddl_parts.append(\",\\n\".join(column_definitions))\n",
    "    ddl_parts.append(\");\")\n",
    "    ddl_parts.append(\"\")\n",
    "    ddl_parts.append(f\"COMMENT ON TABLE {full_table_name} IS 'Camada SILVER: tabela única com dados de atrasos de voos limpos e enriquecidos.';\")\n",
    "    ddl_parts.append(\"\")\n",
    "    ddl_parts.extend(column_comments)\n",
    "    \n",
    "    return \"\\n\".join(ddl_parts)\n",
    "\n",
    "# Gerar DDL\n",
    "ddl_sql = generate_ddl(df, 'silver', 'silver_airline_on_time')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"3.1 Gerando DDL\")\n",
    "print(\"=\" * 80)\n",
    "print(ddl_sql)\n",
    "\n",
    "# Salvar DDL em arquivo\n",
    "ddl_path = '../Data Layer/silver/ddl.sql'\n",
    "os.makedirs(os.path.dirname(ddl_path), exist_ok=True)\n",
    "with open(ddl_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(ddl_sql)\n",
    "print(f\"\\nDDL salvo em: {ddl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_postgres(df: pd.DataFrame, engine, schema: str = 'silver', table_name: str = 'silver_airline_on_time'):\n",
    "    \"\"\"\n",
    "    Carrega DataFrame no PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a ser carregado\n",
    "        engine: SQLAlchemy engine\n",
    "        schema: Nome do schema\n",
    "        table_name: Nome da tabela\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"3.2 Criando tabela e carregando dados\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        statements = [s.strip() for s in ddl_sql.split(';') if s.strip()]\n",
    "        for stmt in statements:\n",
    "            if stmt:\n",
    "                conn.execute(text(stmt))\n",
    "        conn.commit()\n",
    "        print(f\"Tabela {schema}.{table_name} criada/verificada\")\n",
    "\n",
    "    print(f\"Carregando {len(df):,} linhas...\")\n",
    "\n",
    "    df_clean = df.replace({np.nan: None})\n",
    "    \n",
    "    df_clean.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        schema=schema,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        chunksize=5000,\n",
    "        method='multi'\n",
    "    )\n",
    "    \n",
    "    print(\"Dados carregados com sucesso!\")\n",
    "\n",
    "load_to_postgres(df, engine, 'silver', 'silver_airline_on_time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289655e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar índices\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3.3 Criando índices\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "index_columns = []\n",
    "if 'year' in df.columns:\n",
    "    index_columns.append('year')\n",
    "if 'month' in df.columns:\n",
    "    index_columns.append('month')\n",
    "if 'carrier' in df.columns:\n",
    "    index_columns.append('carrier')\n",
    "if 'airport' in df.columns:\n",
    "    index_columns.append('airport')\n",
    "if 'flight_date' in df.columns:\n",
    "    index_columns.append('flight_date')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for col in index_columns:\n",
    "        index_name = f\"idx_{col}\"\n",
    "        try:\n",
    "            conn.execute(text(f\"CREATE INDEX IF NOT EXISTS {index_name} ON silver.silver_airline_on_time ({col});\"))\n",
    "            conn.commit()\n",
    "            print(f\"Indice criado: {index_name} em {col}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar indice {index_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificações pós-carga\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3.4 Verificações pós-carga\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # COUNT(*)\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM silver.silver_airline_on_time;\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"Total de registros na tabela: {count:,}\")\n",
    "    \n",
    "    # Primeiras 5 linhas\n",
    "    print(f\"\\nPrimeiras 5 linhas:\")\n",
    "    result = conn.execute(text(\"SELECT * FROM silver.silver_airline_on_time LIMIT 5;\"))\n",
    "    df_sample = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    display(df_sample)\n",
    "    \n",
    "    # Agregação por year e month (se existirem)\n",
    "    if 'year' in df.columns and 'month' in df.columns:\n",
    "        print(f\"\\nAgregacao por year e month (primeiros 12):\")\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT year, month, COUNT(*) as total\n",
    "            FROM silver.silver_airline_on_time\n",
    "            GROUP BY year, month\n",
    "            ORDER BY year, month\n",
    "            LIMIT 12;\n",
    "        \"\"\"))\n",
    "        df_agg = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "        display(df_agg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e6b74",
   "metadata": {},
   "source": [
    "## 4. Validação Pós-Carga\n",
    "\n",
    "### O que fazemos aqui?\n",
    "\n",
    "Validamos que os dados foram carregados corretamente comparando:\n",
    "- Número de linhas no DataFrame vs PostgreSQL\n",
    "- Colunas no DataFrame vs colunas na tabela\n",
    "- Nulos por coluna\n",
    "- Min/max de métricas numéricas importantes\n",
    "\n",
    "### Por que é importante?\n",
    "\n",
    "- Garantir integridade dos dados após o load\n",
    "- Detectar problemas de conversão de tipos\n",
    "- Validar que não houve perda de dados\n",
    "- Documentar qualidade dos dados na Silver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_load(df: pd.DataFrame, engine, schema: str = 'silver', table_name: str = 'silver_airline_on_time'):\n",
    "    \"\"\"\n",
    "    Valida o carregamento comparando DataFrame e tabela PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        engine: SQLAlchemy engine\n",
    "        schema: Nome do schema\n",
    "        table_name: Nome da tabela\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ETAPA 4: VALIDACAO POS-CARGA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    full_table_name = f\"{schema}.{table_name}\"\n",
    "    \n",
    "    # 1. Comparar número de linhas\n",
    "    print(\"\\n1. Comparacao de linhas:\")\n",
    "    df_rows = len(df)\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {full_table_name};\"))\n",
    "        pg_rows = result.fetchone()[0]\n",
    "    \n",
    "    print(f\"   DataFrame: {df_rows:,} linhas\")\n",
    "    print(f\"   PostgreSQL: {pg_rows:,} linhas\")\n",
    "    if df_rows == pg_rows:\n",
    "        print(\"   Numero de linhas coincide!\")\n",
    "    else:\n",
    "        print(f\"   Diferenca de {abs(df_rows - pg_rows):,} linhas\")\n",
    "    \n",
    "    # 2. Comparar colunas\n",
    "    print(\"\\n2. Comparacao de colunas:\")\n",
    "    df_cols = set(df.columns)\n",
    "    with engine.connect() as conn:\n",
    "        inspector = inspect(engine)\n",
    "        pg_cols = set([col['name'] for col in inspector.get_columns(table_name, schema=schema)])\n",
    "    \n",
    "    print(f\"   DataFrame: {len(df_cols)} colunas\")\n",
    "    print(f\"   PostgreSQL: {len(pg_cols)} colunas\")\n",
    "    \n",
    "    missing_in_pg = df_cols - pg_cols\n",
    "    extra_in_pg = pg_cols - df_cols\n",
    "    \n",
    "    if not missing_in_pg and not extra_in_pg:\n",
    "        print(\"   Todas as colunas coincidem!\")\n",
    "    else:\n",
    "        if missing_in_pg:\n",
    "            print(f\"   Colunas no DataFrame mas nao no PG: {missing_in_pg}\")\n",
    "        if extra_in_pg:\n",
    "            print(f\"   Colunas no PG mas nao no DataFrame: {extra_in_pg}\")\n",
    "    \n",
    "    # 3. Nulos por coluna (amostra)\n",
    "    print(\"\\n3. Nulos por coluna (top 10 com mais nulos):\")\n",
    "    null_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "    null_counts = null_counts[null_counts > 0].head(10)\n",
    "    if len(null_counts) > 0:\n",
    "        for col, count in null_counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"   {col}: {count:,} ({pct:.2f}%)\")\n",
    "    else:\n",
    "        print(\"   Nenhum nulo encontrado nas principais colunas\")\n",
    "    \n",
    "    # 4. Min/Max de métricas numéricas\n",
    "    print(\"\\n4. Min/Max de metricas numericas:\")\n",
    "    metric_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                   if any(pattern in col for pattern in ['_delay', '_flights', '_ct'])]\n",
    "    \n",
    "    for col in metric_cols[:5]:  # Apenas primeiras 5\n",
    "        if col in df.columns:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            print(f\"   {col}: min={min_val:,.2f}, max={max_val:,.2f}\")\n",
    "\n",
    "validate_load(df, engine, 'silver', 'silver_airline_on_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96ea0d",
   "metadata": {},
   "source": [
    "## 5. Relatório Final do ETL\n",
    "\n",
    "### O que foi transformado?\n",
    "\n",
    "Este relatório resume todas as transformações aplicadas durante o processo ETL, permitindo rastreabilidade e auditoria.\n",
    "\n",
    "### Por que é importante?\n",
    "\n",
    "- **Rastreabilidade**: Documenta exatamente o que foi feito\n",
    "- **Auditoria**: Permite verificar qualidade e completude\n",
    "- **Debugging**: Facilita identificar problemas no pipeline\n",
    "- **Melhoria contínua**: Base para otimizações futuras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ae9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RELATÓRIO FINAL DO ETL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar DataFrame de resumo\n",
    "resumo_data = {\n",
    "    'Métrica': [\n",
    "        'Linhas Inicial (RAW)',\n",
    "        'Linhas Final (SILVER)',\n",
    "        'Linhas Removidas (Duplicados)',\n",
    "        'Colunas Inicial',\n",
    "        'Colunas Final',\n",
    "        'Colunas Removidas (100% Nulas)',\n",
    "        'Outliers Detectados (arr_delay)',\n",
    "        'Colunas com Nulos Preenchidos',\n",
    "        'Colunas com Valores Negativos Corrigidos'\n",
    "    ],\n",
    "    'Valor': [\n",
    "        f\"{etl_report['linhas_inicial']:,}\",\n",
    "        f\"{etl_report['linhas_final']:,}\",\n",
    "        f\"{etl_report['duplicados_removidos']:,}\",\n",
    "        etl_report['colunas_inicial'],\n",
    "        etl_report['colunas_final'],\n",
    "        len(etl_report['colunas_removidas']),\n",
    "        f\"{etl_report['outliers_detectados']:,}\",\n",
    "        len(etl_report['colunas_nulos_preenchidos']),\n",
    "        len(etl_report['valores_negativos_corrigidos'])\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_resumo = pd.DataFrame(resumo_data)\n",
    "display(df_resumo)\n",
    "\n",
    "# Detalhes adicionais\n",
    "if etl_report['colunas_removidas']:\n",
    "    print(f\"\\nColunas removidas: {etl_report['colunas_removidas']}\")\n",
    "\n",
    "if etl_report['colunas_nulos_preenchidos']:\n",
    "    print(f\"\\nColunas com nulos preenchidos:\")\n",
    "    for col, count in etl_report['colunas_nulos_preenchidos'].items():\n",
    "        print(f\"   - {col}: {count:,} nulos preenchidos com 0\")\n",
    "\n",
    "if etl_report['valores_negativos_corrigidos']:\n",
    "    print(f\"\\nColunas com valores negativos corrigidos:\")\n",
    "    for col, count in etl_report['valores_negativos_corrigidos'].items():\n",
    "        print(f\"   - {col}: {count:,} valores negativos corrigidos para 0\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETL RAW -> SILVER CONCLUIDO COM SUCESSO!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
